Contrastive Divergence (CD)

Contrastive divergence is a crucial component in unsupervised learning, specifically in training RBMs.
Its learning process is done by approximating the gradient needed to update the weights in these models.

Contrastive Divergence is an iterative algorithm used to train RBMs. The main goal is to estimate the gradient
of the log-likelihood function associated with the RBM. Log-likelihood helps to measure how well a model explains
the data, by simplifying calculations and is widely used to adapt the model during training.

Essentially, CD is used to grasp RBMs briefly. RBMs consist of visible and hidden layers where nodes within layers
are interconnected but do not have connections within the same layer. CD operates by updateing the weights of these
connections to minimize the difference between the observed data and the reconstructed data generated by the RBM.

Similarly, it attempts to reach the lowest / minimal energy state possible when processing input, this determines if it has reached a
satisfactory answer.
